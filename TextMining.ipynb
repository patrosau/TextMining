{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import operator\n",
    "from nltk.corpus import stopwords as sw\n",
    "\n",
    "directory='/home/patrosau@alumno.upv.es/Descargas/hispatweets/'\n",
    "\n",
    "def getFileName(directory, country, userId):\n",
    "    return directory + country + '/' + user + '.json'\n",
    "\n",
    "# Diccionario por pais y sexo de los ficheros de usuario\n",
    "countrydic = dict()\n",
    "with open(directory + 'training.txt') as trainingFile:\n",
    "    for line in trainingFile:\n",
    "        [user, country, sex] = line.split(':::')\n",
    "        file_Name = getFileName(directory, country, user)\n",
    "        if (country in countrydic):\n",
    "            if (sex in countrydic[country]):\n",
    "                countrydic[country][sex].append(file_Name)\n",
    "            else:\n",
    "                countrydic[country][sex] = list()\n",
    "                countrydic[country][sex].append(file_Name)\n",
    "        else:\n",
    "            countrydic[country] = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count num users per country and sex\n",
    "for country in countrydic:\n",
    "     for sex in countrydic[country]:\n",
    "         print (country + ',' + sex + ',' + str(len(countrydic[country][sex])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosostros', 'vosostras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
     ]
    }
   ],
   "source": [
    "sw_spanish = sw.words('spanish')\n",
    "print(sw_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def tokenizeTweet(tweet):\n",
    "\n",
    "    # eliminamos mencion de usuarios, quitamos las repeticiones de mas de 3 letras y pasamos a minusculas\n",
    "    tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=False)\n",
    "    \n",
    "    return tokenizer.tokenize(tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def IsRelevantWord(word):\n",
    "    r = re.compile('\\w+')\n",
    "    \n",
    "    if (word not in sw_spanish):\n",
    "        if (r.match(word)):\n",
    "            return 1;\n",
    "    return 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calcula frecuencias de aparicion de palabras en los tweets de entrada\n",
    "def getFrequentWords(files):\n",
    "\n",
    "    sw_spanish = sw.words('spanish')\n",
    "    d = dict()\n",
    "\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            for line in f:\n",
    "                tokens = tokenizeTweet(json.loads(line)['text'])\n",
    "                for word in tokens:\n",
    "                    if (IsRelevantWord(word)):\n",
    "                        d[word] = d.get(word, 0) + 1\n",
    "\n",
    "    return sorted(d.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFilesForCountry(country):\n",
    "    files = list()\n",
    "    for sex in countrydic[country]:\n",
    "        for file in countrydic[country][sex]:\n",
    "            files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ver la frecuencia de palabras utilizadas por cada pais\n",
    "frequentWordsPerCountry = dict()\n",
    "for country in countrydic:\n",
    "    frequentWordsPerCountry[country] = getFrequentWords(getFilesForCountry(country))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wordExistInOtherCountries(inWord, inCountry):\n",
    "    for country in frequentWordsPerCountry:\n",
    "        if (country != inCountry):\n",
    "            for word in frequentWordsPerCountry[country][:500]:\n",
    "                if (word[0] == inWord ):\n",
    "                    return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# de las 500 palabras mas utilizadas de cada pais ver cuales no aparecen en los otros paises (tarda mucho)\n",
    "distinctWordsPerCountry = dict()\n",
    "for country in frequentWordsPerCountry:\n",
    "    distinctWordsPerCountry[country] = list()\n",
    "    for word in frequentWordsPerCountry[country][:500]:\n",
    "        if (wordExistInOtherCountries(word[0], country) == 0):\n",
    "            distinctWordsPerCountry[country].append(word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bolsaPalabras = list()\n",
    "for country in distinctWordsPerCountry:\n",
    "    for word in distinctWordsPerCountry[country]:\n",
    "        bolsaPalabras.append(word)\n",
    "len(bolsaPalabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# guardamos las palabras que voy a user para la bolsa de palabras en un fichero de texto\n",
    "with open('bolsaPalabras.txt', 'w') as fh:\n",
    "        for word in bolsaPalabras:\n",
    "            fh.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mostUsedWord' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-374b5cb62a17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmostUsedWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetFrequentWords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAllFiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmostUsedWord\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mostUsedWord' is not defined"
     ]
    }
   ],
   "source": [
    "# ver las palabras mas usadas en general (para todos los paises)\n",
    "AllFiles = list()\n",
    "for country in countrydic:\n",
    "    for sex in countrydic[country]:\n",
    "        for file in countrydic[country][sex]:\n",
    "            AllFiles.append(file)\n",
    "\n",
    "mostUsedWords = getFrequentWords(AllFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('si', 114015)\n",
      "('in', 91646)\n",
      "('hoy', 73927)\n",
      "('q', 72204)\n",
      "('gracias', 62388)\n",
      "('día', 59493)\n",
      "('at', 49406)\n",
      "('mas', 49268)\n",
      "('mejor', 45916)\n",
      "('quiero', 44245)\n",
      "('vida', 44177)\n",
      "('ser', 41595)\n",
      "(\"i'm\", 41110)\n",
      "('siempre', 37694)\n",
      "('bien', 37680)\n",
      "('ahora', 36861)\n",
      "('feliz', 35819)\n",
      "('así', 35735)\n",
      "('ver', 35499)\n",
      "('vía', 34107)\n",
      "('the', 33302)\n",
      "('jajaja', 33210)\n",
      "('solo', 32820)\n",
      "('hace', 30455)\n",
      "('mañana', 29886)\n",
      "('tan', 29806)\n",
      "('amor', 28574)\n",
      "('días', 28424)\n",
      "('dios', 27940)\n",
      "('va', 27711)\n",
      "('voy', 27397)\n",
      "('vamos', 27056)\n",
      "('3', 26982)\n",
      "('d', 26714)\n",
      "('hacer', 26480)\n",
      "('foto', 26367)\n",
      "('buenos', 25958)\n",
      "('2', 25355)\n",
      "('gente', 25185)\n",
      "('i', 24254)\n",
      "('casa', 24141)\n",
      "('1', 24128)\n",
      "('noche', 23739)\n",
      "('bueno', 23339)\n",
      "('semana', 23256)\n",
      "('nuevo', 23190)\n",
      "('vez', 23184)\n",
      "('jajajaja', 22581)\n",
      "('nunca', 22473)\n",
      "('buen', 21497)\n",
      "('mundo', 21161)\n",
      "('cosas', 20837)\n",
      "('puede', 20684)\n",
      "('años', 20407)\n",
      "('amo', 20231)\n",
      "('gran', 20207)\n",
      "('tiempo', 20170)\n",
      "('fin', 19601)\n",
      "('mal', 19592)\n",
      "('ir', 18843)\n",
      "('buena', 18611)\n",
      "('aquí', 18388)\n",
      "('rt', 18376)\n",
      "('cada', 18279)\n",
      "('lima', 18094)\n",
      "('video', 18085)\n",
      "('año', 17964)\n",
      "('dos', 17835)\n",
      "('menos', 17747)\n",
      "('san', 17484)\n",
      "('creo', 16898)\n",
      "('x', 16640)\n",
      "('alguien', 16505)\n",
      "('xd', 16422)\n",
      "('you', 16217)\n",
      "('da', 15910)\n",
      "('puedo', 15892)\n",
      "('dia', 15820)\n",
      "('acabo', 15814)\n",
      "('pues', 15629)\n",
      "('tener', 15507)\n",
      "('hora', 15451)\n",
      "('jaja', 15375)\n",
      "('to', 14870)\n",
      "('después', 14728)\n",
      "('buenas', 14321)\n",
      "('santiago', 14316)\n",
      "('dormir', 14283)\n",
      "('nadie', 14137)\n",
      "('w', 14076)\n",
      "('trabajo', 13996)\n",
      "('tarde', 13983)\n",
      "('via', 13913)\n",
      "('and', 13800)\n",
      "('mismo', 13775)\n",
      "('verdad', 13731)\n",
      "('5', 13464)\n",
      "('toda', 13446)\n",
      "('región', 13338)\n",
      "('dice', 13305)\n"
     ]
    }
   ],
   "source": [
    "for word in mostUsedWords[:100]:\n",
    "    print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('bolsaPalabras.txt', 'a') as fh:\n",
    "        for word in mostUsedWords[:500]:\n",
    "            fh.write(word[0] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lima\n",
      "perú\n",
      "miraflores\n",
      "peru\n",
      "surco\n",
      "isidro\n",
      "callao\n",
      "besitos\n",
      "cercado\n",
      "peruano\n"
     ]
    }
   ],
   "source": [
    "bolsaPalabras = list()\n",
    "with open('bolsaPalabras.txt') as palabras:\n",
    "    for palabra in palabras:\n",
    "        bolsaPalabras.append(palabra.rstrip())\n",
    "len(bolsaPalabras)\n",
    "for p in bolsaPalabras[:10]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "961"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bolsaPalabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def palabraIsUsed(palabra, uw):\n",
    "    for p in uw:\n",
    "        if p[0] == palabra:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getFeaturesList(fileName):\n",
    "    res = list()\n",
    "    palabrasUsadas = getFrequentWords(fileName)\n",
    "    for palabra in bolsaPalabras:\n",
    "            if palabraIsUsed(palabra,palabrasUsadas):\n",
    "                res.append(1)\n",
    "            else:\n",
    "                res.append(0)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTrainingData(user, country):\n",
    "    listFiles = list()\n",
    "    listFiles.append(getFileName(directory, country, user))\n",
    "    userVector = getFeaturesList(listFiles)\n",
    "    return userVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(directory + 'trainingdata.txt', 'w') as train:\n",
    "    with open(directory + 'training.txt') as trainingFile:\n",
    "        for line in trainingFile:\n",
    "            [user, country, sex] = line.split(':::')\n",
    "            train.write(user + ',')\n",
    "            userVector = getTrainingData(user, country)\n",
    "            for feature in userVector:\n",
    "                train.write(str(feature) + ',')\n",
    "            train.write(country + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(directory + 'testdata.txt', 'w') as train:\n",
    "    with open(directory + 'test.txt') as trainingFile:\n",
    "        for line in trainingFile:\n",
    "            [user, country, sex] = line.split(':::')\n",
    "            train.write(user + ',')\n",
    "            userVector = getTrainingData(user, country)\n",
    "            for feature in userVector:\n",
    "                train.write(str(feature) + ',')\n",
    "            train.write(country + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
