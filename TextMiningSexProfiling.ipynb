{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import operator\n",
    "from nltk.corpus import stopwords as sw\n",
    "\n",
    "directory='/home/patrosau@alumno.upv.es/Descargas/hispatweets/'\n",
    "\n",
    "def getFileName(directory, country, userId):\n",
    "    return directory + country + '/' + user + '.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def tokenizeTweet(tweet):\n",
    "\n",
    "    # eliminamos mencion de usuarios, quitamos las repeticiones de mas de 3 letras y pasamos a minusculas\n",
    "    tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=False)\n",
    "    \n",
    "    return tokenizer.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spanish_tagged_words = nltk.corpus.conll2002.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sao', 'NC')\n",
      "('Paulo', 'VMI')\n",
      "('(', 'Fpa')\n",
      "('Brasil', 'NC')\n",
      "(')', 'Fpt')\n",
      "(',', 'Fc')\n",
      "('23', 'Z')\n",
      "('may', 'NC')\n",
      "('(', 'Fpa')\n",
      "('EFECOM', 'NP')\n"
     ]
    }
   ],
   "source": [
    "for tagged_words in spanish_tagged_words[:10]:\n",
    "    print (tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import cess_esp\n",
    "corpus_sentences=cess_esp.tagged_sents()\n",
    "\n",
    "words_tagged = dict()\n",
    "for sentence in corpus_sentences:\n",
    "    for token_tag in sentence:\n",
    "        if (token_tag[0] not in words_tagged):\n",
    "            words_tagged[token_tag[0]] = token_tag[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pronombres = ['mia', 'mio', 'yo', 'mi', 'me', 'tu', 'te', 'ti', 'contigo', 'usted', 'él', 'ella', 'consigo', 'nosotros','nosotras', 'nos', 'vosotros', 'vosotras', 'os', 'vos', 'ellos', 'ellas',\n",
    "             'mío','tuyo' ,'suyo','nuestro','vuestro','suyo', 'míos' ,'tuyos','suyos','nuestros','vuestros','suyos', 'mía' ,'tuya' ,'suya','nuestra','vuestra','suya',\n",
    "             'mías','tuyas','suyas' ,'nuestras' ,'vuestras','suyas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTag(word):\n",
    "    if word in words_tagged:\n",
    "        return words_tagged[word]\n",
    "    else:\n",
    "        if word in pronombres:\n",
    "            return 'p'\n",
    "    return '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la da0fs0\n",
      "casa ncfs000\n",
      "es vsip3s0\n",
      "bonita aq0fs0\n",
      "como cs\n",
      "la da0fs0\n",
      "mia p\n",
      "y cc\n",
      "la da0fs0\n",
      "tuya p\n",
      "el da0ms0\n",
      "este dd0ms0\n",
      "yo pp1csn00\n",
      "vale vmip3s0\n",
      "esta dd0fs0\n",
      "esa dd0fs0\n",
      "vuestra p\n"
     ]
    }
   ],
   "source": [
    "text = \"la casa es bonita como la mia y la tuya el este yo vale esta esa vuestra\"\n",
    "tokens = text.split(' ');\n",
    "for token in tokens:\n",
    "    print (token, getTag(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calcula las caracteristicas de un autor\n",
    "# numero de determinantes / normalizado por numero total palabras\n",
    "# numero de adjetivos / normalizado por numero total de palabras\n",
    "# numero de ''de'' / normalizado por numero total de palabras\n",
    "# numero de pronombres / normalizado por numero total de palabras\n",
    "# numero de 'para' / normalizado por numero total de palabras\n",
    "# numero de 'con' / normalizado por numero total de palabras\n",
    "# numero de 'no' / normalizado por numero total de palabras\n",
    "# numero de verbos presente / normalizado por numero total de palabras\n",
    "def getAuthorFeatures(fileName):\n",
    "\n",
    "    wordCount = 0\n",
    "    detCount = 0\n",
    "    adjCount = 0\n",
    "    deCount = 0\n",
    "    pronCount = 0\n",
    "    paraCount = 0\n",
    "    conCount = 0\n",
    "    vbpCount = 0\n",
    "    noCount = 0\n",
    "\n",
    "    \n",
    "    with open(fileName) as f:\n",
    "        for line in f:\n",
    "            tokens = tokenizeTweet(json.loads(line)['text'])\n",
    "            for token in tokens:\n",
    "\n",
    "                wordCount = wordCount + 1\n",
    "                if token == 'de':\n",
    "                    deCount = deCount + 1\n",
    "                elif token == 'con':\n",
    "                    conCount = conCount + 1\n",
    "                elif token == 'para':\n",
    "                    paraCount = paraCount + 1\n",
    "                elif token == 'no':\n",
    "                    noCount = noCount + 1\n",
    "                else:\n",
    "                    tag = getTag(token)\n",
    "                    if tag.startswith('v'):\n",
    "                        vbpCount = vbpCount + 1\n",
    "                    if tag.startswith('d'):\n",
    "                        detCount = detCount + 1\n",
    "                    if tag.startswith('a'):\n",
    "                        adjCount = adjCount + 1\n",
    "                    if tag.startswith('p'):\n",
    "                        pronCount = pronCount + 1\n",
    "\n",
    "    if wordCount > 0:\n",
    "        return [detCount/wordCount, adjCount/wordCount, deCount/wordCount, pronCount/wordCount, paraCount/wordCount, conCount/wordCount, vbpCount/wordCount, noCount/wordCount]\n",
    "    else:\n",
    "        return [0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(directory + 'sextrainingdata.txt', 'w') as train:\n",
    "    with open(directory + 'training.txt') as trainingFile:\n",
    "        for line in trainingFile:\n",
    "            [user, country, sex] = line.split(':::')\n",
    "            train.write(user + ',')\n",
    "            userVector = getAuthorFeatures(getFileName(directory, country, user))\n",
    "            for feature in userVector:\n",
    "                train.write(str(feature) + ',')\n",
    "            train.write(sex + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(directory + 'sextestdata.txt', 'w') as train:\n",
    "    with open(directory + 'test.txt') as trainingFile:\n",
    "        for line in trainingFile:\n",
    "            [user, country, sex] = line.split(':::')\n",
    "            train.write(user + ',')\n",
    "            userVector = getAuthorFeatures(getFileName(directory, country, user))\n",
    "            for feature in userVector:\n",
    "                train.write(str(feature) + ',')\n",
    "            train.write(sex + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
